{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A causal look into the factors of world happiness [2-Causal inference]\n",
    "\n",
    "## intro\n",
    "We will put aside causal graphs and Graphical Causal Modeling as experimental practice (see [previous post](https://economyoftime.net/a-causal-look-into-the-factors-of-world-happiness-1-graphical-modeling-5d6ae3a5df1)) for this time and look into Causal Inference itself as presented by Laurence Wong in his [very interesting blogpost](https://laurencewong.com/software/initialization) from 2016. We are going to apply the methods described there to our dataset.\n",
    "\n",
    "For the sake of this we will have these new mapping:\n",
    "```\n",
    "Y: Ladder score\n",
    "D: \"treatment\" status indicator, we will try to do some interventions\n",
    "   on some subset of the records (for example, what happens if income raises...)\n",
    "   this binary label will be set to 1 for the records that received the intervention.\n",
    "X: a matrix of all the covariates used in the dataset (Y  S  J  X  W)\n",
    "```\n",
    "\n",
    "A score to which has been assigned treatment is `Y(1)` (treated), otherwise non-treated is `Y(0)` (non-treated).\n",
    "\n",
    "This exercise leverages the `causalinference` Python library.\n",
    "\n",
    "## load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from causalinference import CausalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country name</th>\n",
       "      <th>Regional indicator</th>\n",
       "      <th>Ladder score</th>\n",
       "      <th>Standard error of ladder score</th>\n",
       "      <th>upperwhisker</th>\n",
       "      <th>lowerwhisker</th>\n",
       "      <th>Logged GDP per capita</th>\n",
       "      <th>Social support</th>\n",
       "      <th>Healthy life expectancy</th>\n",
       "      <th>Freedom to make life choices</th>\n",
       "      <th>Generosity</th>\n",
       "      <th>Perceptions of corruption</th>\n",
       "      <th>Ladder score in Dystopia</th>\n",
       "      <th>Dystopia + residual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Finland</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>7.842</td>\n",
       "      <td>0.032</td>\n",
       "      <td>7.904</td>\n",
       "      <td>7.780</td>\n",
       "      <td>10.775</td>\n",
       "      <td>0.954</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.949</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>0.186</td>\n",
       "      <td>2.43</td>\n",
       "      <td>3.253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Denmark</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>7.620</td>\n",
       "      <td>0.035</td>\n",
       "      <td>7.687</td>\n",
       "      <td>7.552</td>\n",
       "      <td>10.933</td>\n",
       "      <td>0.954</td>\n",
       "      <td>72.7</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.179</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Switzerland</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>7.571</td>\n",
       "      <td>0.036</td>\n",
       "      <td>7.643</td>\n",
       "      <td>7.500</td>\n",
       "      <td>11.117</td>\n",
       "      <td>0.942</td>\n",
       "      <td>74.4</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.292</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iceland</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>7.554</td>\n",
       "      <td>0.059</td>\n",
       "      <td>7.670</td>\n",
       "      <td>7.438</td>\n",
       "      <td>10.878</td>\n",
       "      <td>0.983</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.673</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Netherlands</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>7.464</td>\n",
       "      <td>0.027</td>\n",
       "      <td>7.518</td>\n",
       "      <td>7.410</td>\n",
       "      <td>10.932</td>\n",
       "      <td>0.942</td>\n",
       "      <td>72.4</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.338</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Country name Regional indicator  Ladder score  \\\n",
       "0      Finland     Western Europe         7.842   \n",
       "1      Denmark     Western Europe         7.620   \n",
       "2  Switzerland     Western Europe         7.571   \n",
       "3      Iceland     Western Europe         7.554   \n",
       "4  Netherlands     Western Europe         7.464   \n",
       "\n",
       "   Standard error of ladder score  upperwhisker  lowerwhisker  \\\n",
       "0                           0.032         7.904         7.780   \n",
       "1                           0.035         7.687         7.552   \n",
       "2                           0.036         7.643         7.500   \n",
       "3                           0.059         7.670         7.438   \n",
       "4                           0.027         7.518         7.410   \n",
       "\n",
       "   Logged GDP per capita  Social support  Healthy life expectancy  \\\n",
       "0                 10.775           0.954                     72.0   \n",
       "1                 10.933           0.954                     72.7   \n",
       "2                 11.117           0.942                     74.4   \n",
       "3                 10.878           0.983                     73.0   \n",
       "4                 10.932           0.942                     72.4   \n",
       "\n",
       "   Freedom to make life choices  Generosity  Perceptions of corruption  \\\n",
       "0                         0.949      -0.098                      0.186   \n",
       "1                         0.946       0.030                      0.179   \n",
       "2                         0.919       0.025                      0.292   \n",
       "3                         0.955       0.160                      0.673   \n",
       "4                         0.913       0.175                      0.338   \n",
       "\n",
       "   Ladder score in Dystopia  Dystopia + residual  \n",
       "0                      2.43                3.253  \n",
       "1                      2.43                2.868  \n",
       "2                      2.43                2.839  \n",
       "3                      2.43                2.967  \n",
       "4                      2.43                2.798  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('world-happiness-report-2021.csv')\n",
    "df.drop(list(df.filter(regex='Explained')), axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>S</th>\n",
       "      <th>J</th>\n",
       "      <th>X</th>\n",
       "      <th>W</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.842</td>\n",
       "      <td>10.775</td>\n",
       "      <td>0.954</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.620</td>\n",
       "      <td>10.933</td>\n",
       "      <td>0.954</td>\n",
       "      <td>72.7</td>\n",
       "      <td>0.946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.571</td>\n",
       "      <td>11.117</td>\n",
       "      <td>0.942</td>\n",
       "      <td>74.4</td>\n",
       "      <td>0.919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.554</td>\n",
       "      <td>10.878</td>\n",
       "      <td>0.983</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.464</td>\n",
       "      <td>10.932</td>\n",
       "      <td>0.942</td>\n",
       "      <td>72.4</td>\n",
       "      <td>0.913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Y       S      J     X      W\n",
       "0  7.842  10.775  0.954  72.0  0.949\n",
       "1  7.620  10.933  0.954  72.7  0.946\n",
       "2  7.571  11.117  0.942  74.4  0.919\n",
       "3  7.554  10.878  0.983  73.0  0.955\n",
       "4  7.464  10.932  0.942  72.4  0.913"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[[\"Ladder score\",\n",
    "         \"Logged GDP per capita\",\n",
    "         \"Social support\",\n",
    "         \"Healthy life expectancy\",\n",
    "         \"Freedom to make life choices\"]\n",
    "].copy()\n",
    "df.rename(columns={\n",
    "    \"Ladder score\": \"Y\",\n",
    "    \"Logged GDP per capita\": \"S\",\n",
    "    \"Social support\": \"J\",\n",
    "    \"Healthy life expectancy\": \"X\",\n",
    "    \"Freedom to make life choices\": \"W\" \n",
    "}, inplace=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>S</th>\n",
       "      <th>J</th>\n",
       "      <th>X</th>\n",
       "      <th>W</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>149.000000</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>149.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.532839</td>\n",
       "      <td>9.432208</td>\n",
       "      <td>0.814745</td>\n",
       "      <td>64.992799</td>\n",
       "      <td>0.791597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.073924</td>\n",
       "      <td>1.158601</td>\n",
       "      <td>0.114889</td>\n",
       "      <td>6.762043</td>\n",
       "      <td>0.113332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.523000</td>\n",
       "      <td>6.635000</td>\n",
       "      <td>0.463000</td>\n",
       "      <td>48.478000</td>\n",
       "      <td>0.382000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.852000</td>\n",
       "      <td>8.541000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>59.802000</td>\n",
       "      <td>0.718000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.534000</td>\n",
       "      <td>9.569000</td>\n",
       "      <td>0.832000</td>\n",
       "      <td>66.603000</td>\n",
       "      <td>0.804000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.255000</td>\n",
       "      <td>10.421000</td>\n",
       "      <td>0.905000</td>\n",
       "      <td>69.600000</td>\n",
       "      <td>0.877000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.842000</td>\n",
       "      <td>11.647000</td>\n",
       "      <td>0.983000</td>\n",
       "      <td>76.953000</td>\n",
       "      <td>0.970000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Y           S           J           X           W\n",
       "count  149.000000  149.000000  149.000000  149.000000  149.000000\n",
       "mean     5.532839    9.432208    0.814745   64.992799    0.791597\n",
       "std      1.073924    1.158601    0.114889    6.762043    0.113332\n",
       "min      2.523000    6.635000    0.463000   48.478000    0.382000\n",
       "25%      4.852000    8.541000    0.750000   59.802000    0.718000\n",
       "50%      5.534000    9.569000    0.832000   66.603000    0.804000\n",
       "75%      6.255000   10.421000    0.905000   69.600000    0.877000\n",
       "max      7.842000   11.647000    0.983000   76.953000    0.970000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assumptions\n",
    "These prerequisites must hold:\n",
    "\n",
    "1. *randomized experiment* (\"strong\" prerequisite)\n",
    "\n",
    "assignment of treatment must be random:\n",
    "```\n",
    "(Y(0), Y(1)) ⟂ D\n",
    "```\n",
    "\n",
    "2. *unconfounded assumption* (\"weak\" prerequisite)\n",
    "\n",
    "exclude confounding among covariates (`X`), there is no unobserved confounder:\n",
    "```\n",
    "(Y(0), Y(1)) ⟂ D|X\n",
    "```\n",
    "Effects of treatment are orthogonal to treatment conditional covariates.\n",
    "\n",
    "\n",
    "Spotting confounders is not the subject of this post, see Causal Discovery and Causal Graphs about how to avoid confounding.\n",
    "\n",
    "## initialisation\n",
    "\n",
    "Let's assign each record to a \"treated group\" (`Y(1)`) or to a \"control group\" (`Y(0)`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 0 0 1 0 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0\n",
      " 0 0 1 0 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 0 0\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "# randomise treatment in the dataset\n",
    "import numpy as np\n",
    "df[\"D\"] = np.random.choice(a=[0,1], size=df[\"Y\"].count(), p=[0.4, 0.6])\n",
    "print(df[\"D\"].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## causalinference uses the convention of calling covariates as `Xn` so we rename for convenience\n",
    "df.rename(columns={\n",
    "    \"S\": \"X0\",\n",
    "    \"J\": \"X1\",\n",
    "    \"X\": \"X2\",\n",
    "    \"W\": \"X3\" \n",
    "}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## intervention 1\n",
    "\n",
    "Now we simulate the treatment, we increase the \"freedom of choice\" index (`W` or `X3` depending on which convention we are using) of a given amount only for the treated samples.\n",
    "\n",
    "First we need to find a way to do that with incurring in errors, let's see how `W` looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    149.000000\n",
       "mean       0.791597\n",
       "std        0.113332\n",
       "min        0.382000\n",
       "25%        0.718000\n",
       "50%        0.804000\n",
       "75%        0.877000\n",
       "max        0.970000\n",
       "Name: X3, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"X3\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a standard deviation of `0.113332` so we will go for 1/10 of that just to be sure we don't stir the water too much in the beginning. So our treatment looks like:\n",
    "```\n",
    "Y(1) => X3 = X3 + (std(X3) / 10)\n",
    "```\n",
    "\n",
    "this is the starting scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.842</td>\n",
       "      <td>10.775</td>\n",
       "      <td>0.954</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.949</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.620</td>\n",
       "      <td>10.933</td>\n",
       "      <td>0.954</td>\n",
       "      <td>72.7</td>\n",
       "      <td>0.946</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.571</td>\n",
       "      <td>11.117</td>\n",
       "      <td>0.942</td>\n",
       "      <td>74.4</td>\n",
       "      <td>0.919</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.554</td>\n",
       "      <td>10.878</td>\n",
       "      <td>0.983</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.955</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.464</td>\n",
       "      <td>10.932</td>\n",
       "      <td>0.942</td>\n",
       "      <td>72.4</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Y      X0     X1    X2     X3  D\n",
       "0  7.842  10.775  0.954  72.0  0.949  1\n",
       "1  7.620  10.933  0.954  72.7  0.946  1\n",
       "2  7.571  11.117  0.942  74.4  0.919  1\n",
       "3  7.554  10.878  0.983  73.0  0.955  1\n",
       "4  7.464  10.932  0.942  72.4  0.913  0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's set aside the initial data and its causal model for comparison\n",
    "df_start = df.copy()\n",
    "df_start.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is after we apply the treatment (\"**intervention 1**\": slight increase of freedom of choice):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011333178506605257\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.842</td>\n",
       "      <td>10.775</td>\n",
       "      <td>0.954</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.960333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.620</td>\n",
       "      <td>10.933</td>\n",
       "      <td>0.954</td>\n",
       "      <td>72.7</td>\n",
       "      <td>0.957333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.571</td>\n",
       "      <td>11.117</td>\n",
       "      <td>0.942</td>\n",
       "      <td>74.4</td>\n",
       "      <td>0.930333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.554</td>\n",
       "      <td>10.878</td>\n",
       "      <td>0.983</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.966333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.464</td>\n",
       "      <td>10.932</td>\n",
       "      <td>0.942</td>\n",
       "      <td>72.4</td>\n",
       "      <td>0.913000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Y      X0     X1    X2        X3  D\n",
       "0  7.842  10.775  0.954  72.0  0.960333  1\n",
       "1  7.620  10.933  0.954  72.7  0.957333  1\n",
       "2  7.571  11.117  0.942  74.4  0.930333  1\n",
       "3  7.554  10.878  0.983  73.0  0.966333  1\n",
       "4  7.464  10.932  0.942  72.4  0.913000  0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_dev_X3 = df_start[\"X3\"].std()\n",
    "print(std_dev_X3 / 10)\n",
    "\n",
    "mask = df_start[\"D\"] == 1\n",
    "\n",
    "df_intervention1 = df_start.copy()\n",
    "# apply intervention\n",
    "df_intervention1.loc[mask, 'X3'] = df_intervention1.loc[mask, \"X3\"].apply(lambda x: x + (std_dev_X3 / 10))\n",
    "df_intervention1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the samples with intervention (`D=1`) have now the `X3` value increased by a minimal ~0.01.\n",
    "\n",
    "Let's generate the model for **intervention 1**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we simplify the model considering only some covariates\n",
    "causal_interv1 = CausalModel(\n",
    "    Y=df_intervention1[\"Y\"].to_numpy(),\n",
    "    D=df_intervention1[\"D\"].to_numpy(),\n",
    "    X=df_intervention1[[\"X0\", \"X1\", \"X2\", \"X3\"]].to_numpy()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ready to roll!\n",
    "\n",
    "Let's see some statistics from the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary Statistics\n",
      "\n",
      "                        Controls (N_c=55)          Treated (N_t=94)             \n",
      "       Variable         Mean         S.d.         Mean         S.d.     Raw-diff\n",
      "--------------------------------------------------------------------------------\n",
      "              Y        5.634        1.042        5.474        1.093       -0.160\n",
      "\n",
      "                        Controls (N_c=55)          Treated (N_t=94)             \n",
      "       Variable         Mean         S.d.         Mean         S.d.     Nor-diff\n",
      "--------------------------------------------------------------------------------\n",
      "             X0        9.674        0.980        9.291        1.234       -0.344\n",
      "             X1        0.842        0.105        0.799        0.118       -0.389\n",
      "             X2       65.715        6.230       64.570        7.053       -0.172\n",
      "             X3        0.788        0.121        0.805        0.109        0.152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(causal_interv1.summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot to go trough here. `N_c` is the size of the control group, `N_t` is the size of the treated group as came out from random sampling. Mean and Standard Deviation are computed for each group for the score and each covariate.\n",
    "\n",
    "In the upper right `Raw-diff` is the expected difference between treated and non-treated. This is an absolute difference `E[Y(1) - Y(0)]`, so it is not much explicative and also in this case the difference is lower than the standard deviation.\n",
    "\n",
    "The interesting part is the normalized differences column `Nor-diff` for each covariate. This is the Imbens-Rubin (2015) normalized difference in average covariates. It is a relatively complicated equation that the library compute for us, quite useful, and this is just the beginning. `Nor-diff` is used to spot \"covariate imbalance\" that happens when treatment and control groups demonstrate insufficient overlap, usually we expect its values to be lower than 0.5 otherwise we may need to apply correction that we are going to explore later like \"trimming\".\n",
    "\n",
    "In this case we don't see much of an effect so we will try to \"increase the dosage\" and push the value to a greater increase.\n",
    "\n",
    "## intervention 2\n",
    "\n",
    "We now apply a treatment that is 1/3 of the freedom of choice index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03777726168868419\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.842</td>\n",
       "      <td>10.775</td>\n",
       "      <td>0.954</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.986777</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.620</td>\n",
       "      <td>10.933</td>\n",
       "      <td>0.954</td>\n",
       "      <td>72.7</td>\n",
       "      <td>0.983777</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.571</td>\n",
       "      <td>11.117</td>\n",
       "      <td>0.942</td>\n",
       "      <td>74.4</td>\n",
       "      <td>0.956777</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.554</td>\n",
       "      <td>10.878</td>\n",
       "      <td>0.983</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.992777</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.464</td>\n",
       "      <td>10.932</td>\n",
       "      <td>0.942</td>\n",
       "      <td>72.4</td>\n",
       "      <td>0.913000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Y      X0     X1    X2        X3  D\n",
       "0  7.842  10.775  0.954  72.0  0.986777  1\n",
       "1  7.620  10.933  0.954  72.7  0.983777  1\n",
       "2  7.571  11.117  0.942  74.4  0.956777  1\n",
       "3  7.554  10.878  0.983  73.0  0.992777  1\n",
       "4  7.464  10.932  0.942  72.4  0.913000  0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_dev_X3 = df_start[\"X3\"].std()\n",
    "print(std_dev_X3 / 3)\n",
    "\n",
    "mask = df_start[\"D\"] == 1\n",
    "\n",
    "df_intervention2 = df_start.copy()\n",
    "df_intervention2.loc[mask, 'X3'] = df_intervention2.loc[mask, \"X3\"].apply(lambda x: x + (std_dev_X3 / 3))\n",
    "df_intervention2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recompute the causal model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary Statistics\n",
      "\n",
      "                        Controls (N_c=55)          Treated (N_t=94)             \n",
      "       Variable         Mean         S.d.         Mean         S.d.     Raw-diff\n",
      "--------------------------------------------------------------------------------\n",
      "              Y        5.634        1.042        5.474        1.093       -0.160\n",
      "\n",
      "                        Controls (N_c=55)          Treated (N_t=94)             \n",
      "       Variable         Mean         S.d.         Mean         S.d.     Nor-diff\n",
      "--------------------------------------------------------------------------------\n",
      "             X0        9.674        0.980        9.291        1.234       -0.344\n",
      "             X1        0.842        0.105        0.799        0.118       -0.389\n",
      "             X2       65.715        6.230       64.570        7.053       -0.172\n",
      "             X3        0.788        0.121        0.832        0.109        0.381\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we simplify the model considering only some covariates\n",
    "causal_interv2 = CausalModel(\n",
    "    Y=df_intervention2[\"Y\"].to_numpy(),\n",
    "    D=df_intervention2[\"D\"].to_numpy(),\n",
    "    X=df_intervention2[[\"X0\", \"X1\", \"X2\", \"X3\"]].to_numpy()\n",
    ")\n",
    "\n",
    "print(causal_interv2.summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the index `Nor-diff` for covariate imbalance grew relevantly for `X3` quite close to the 0.5 threshold and that exactly what we were expecting, we pushed `X3` so much that a a light on our dashboard started blinking, probably intervention 2 is too much of a treatment and we risk to invalidate the critical next step that is **Treatment Effect Estimation**.\n",
    "\n",
    "## Treatment Effect Estimation\n",
    "\n",
    "In the previous sections we applied two simulated intervention on a covariate, how do we estimate the expected effects of the interventions? `causalinference` gives us some tools, we will look into: \"Ordinary Least Square (**OLS**)\" and \"**Matching**\" to estimate the effect of the intervention on the `Ladder Score`.\n",
    "\n",
    "\n",
    "### OLS\n",
    "Compute OLS for **intervention 1** and **intervention 2**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Treatment Effect Estimates: OLS\n",
      "\n",
      "                     Est.       S.e.          z      P>|z|      [95% Conf. int.]\n",
      "--------------------------------------------------------------------------------\n",
      "           ATE      0.058      0.104      0.553      0.580     -0.147      0.262\n",
      "           ATC      0.044      0.089      0.492      0.623     -0.131      0.219\n",
      "           ATT      0.066      0.119      0.552      0.581     -0.168      0.299\n",
      "\n",
      "\n",
      "Treatment Effect Estimates: OLS\n",
      "\n",
      "                     Est.       S.e.          z      P>|z|      [95% Conf. int.]\n",
      "--------------------------------------------------------------------------------\n",
      "           ATE     -0.001      0.112     -0.008      0.994     -0.221      0.219\n",
      "           ATC     -0.028      0.092     -0.299      0.765     -0.209      0.154\n",
      "           ATT      0.015      0.133      0.111      0.911     -0.246      0.276\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lorenzo/code/notepad/.venv/lib/python3.8/site-packages/causalinference/estimators/ols.py:21: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  olscoef = np.linalg.lstsq(Z, Y)[0]\n"
     ]
    }
   ],
   "source": [
    "causal_interv1.reset()\n",
    "causal_interv1.est_via_ols()\n",
    "print(causal_interv1.estimates)\n",
    "\n",
    "causal_interv2.reset()\n",
    "causal_interv2.est_via_ols()\n",
    "print(causal_interv2.estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS applies a linear function with covariates adjustments.\n",
    "\n",
    "Here we are three foundamental indices that are computed by the estimation:\n",
    "1. ATE is the Average Treatment Effect: the effect observed in the simulation on the whole dataset\n",
    "2. ATC is the Average Effect on Controls: the effect observed in the simulation on the control group\n",
    "3. ATT is the Avergae Effect on Treated: the effect observed in the simulation on treated group\n",
    "\n",
    "> This result shows that OLS is essentially imputing the missing potential outcomes of a given group by extrapolating linearly from the observations of the other group. It thus follows that the less covariate overlap there is between the two groups the more hopelessly heroic the extrapolation, especially if the underlying relationship between outcomes and covariates is nonlinear to begin with. (Laurence Wong)\n",
    "\n",
    "There are plenty of considerations to keep in mind when reading at these numbers. Looking at the column `Est.` you can have a very rough idea of what is going on between the covariate object of the intervention and the effect. Something we can do is to try a different effect estimation algorithm to see if the extrapolation run with OLS can be adjusted somehow.\n",
    "\n",
    "### Matching\n",
    "We can try to compute the estimation on matching records with similar covariate values. We apply the linear extrapolation only on records that have similar characteristics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Treatment Effect Estimates: Matching\n",
      "\n",
      "                     Est.       S.e.          z      P>|z|      [95% Conf. int.]\n",
      "--------------------------------------------------------------------------------\n",
      "           ATE     -0.118      0.151     -0.785      0.433     -0.414      0.177\n",
      "           ATC     -0.020      0.167     -0.119      0.905     -0.348      0.308\n",
      "           ATT     -0.176      0.176     -0.997      0.319     -0.521      0.170\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lorenzo/code/notepad/.venv/lib/python3.8/site-packages/causalinference/estimators/matching.py:100: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  return np.linalg.lstsq(X, Y)[0][1:]  # don't need intercept coef\n"
     ]
    }
   ],
   "source": [
    "causal_interv1.reset()\n",
    "causal_interv1.est_via_matching(bias_adj=True)\n",
    "print(causal_interv1.estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Treatment Effect Estimates: Matching\n",
      "\n",
      "                     Est.       S.e.          z      P>|z|      [95% Conf. int.]\n",
      "--------------------------------------------------------------------------------\n",
      "           ATE     -0.100      0.138     -0.729      0.466     -0.370      0.170\n",
      "           ATC     -0.049      0.170     -0.286      0.775     -0.383      0.285\n",
      "           ATT     -0.131      0.153     -0.854      0.393     -0.430      0.169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "causal_interv2.reset()\n",
    "causal_interv2.est_via_matching(bias_adj=True)\n",
    "print(causal_interv2.estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "We can easily say one thing though, in this estimation we took the whole of the dataset without considering the substantial differences among the records. We may have ended up adding a bias to the estimation by considering all the records at the same time. We are going to apply \"stratification\" so we can see if the effect estimation changes when applied to more homogeneous subgroups in the dataset.\n",
    "\n",
    "## propensity score\n",
    "\n",
    "We are going to create strata in the dataset so to mitigate possible biases. Before we need to introduve the concept of **propensity score**.\n",
    "\n",
    "> The probability of receiving treatment, also known as the propensity score, plays a very special role in the estimation of treatment effects. (Laurence Wong)\n",
    "\n",
    "> The propensity score is the conditional probability of receiving the treatment given the observed covariates. Estimation is done via a logistic regression. (`est_propensity` docstring)\n",
    "\n",
    "Thanks to the unconfounded assumptions (see section above) and the work done by Rosenbaum-Rubin (1983), we  can safely assume that this reasoning follows:\n",
    "```\n",
    "# unconfoundedness assumption\n",
    "(Y(0), Y(1)) ⟂ D\n",
    "# implies according to Rosembaum-Rubin \n",
    "(Y(0), Y(1)) ⟂ D | p(X)\n",
    "# that is\n",
    "p(X) = P(D=1|X)\n",
    "```\n",
    "This allows to define an algorithm Imbens-Rubin(2015) for variable selection for estimating the propensity score. The propensity score is foundamental to refine the estimation techniques for causal effects.\n",
    "\n",
    "Let's try to compute the propensity score for the original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Treatment Effect Estimates: OLS\n",
      "\n",
      "                     Est.       S.e.          z      P>|z|      [95% Conf. int.]\n",
      "--------------------------------------------------------------------------------\n",
      "           ATE      0.083      0.101      0.818      0.413     -0.116      0.281\n",
      "           ATC      0.075      0.089      0.841      0.401     -0.099      0.249\n",
      "           ATT      0.088      0.114      0.771      0.441     -0.135      0.311\n",
      "\n",
      "Treatment Effect Estimates: Matching\n",
      "\n",
      "                     Est.       S.e.          z      P>|z|      [95% Conf. int.]\n",
      "--------------------------------------------------------------------------------\n",
      "           ATE     -0.056      0.153     -0.363      0.717     -0.356      0.245\n",
      "           ATC      0.019      0.178      0.105      0.916     -0.330      0.367\n",
      "           ATT     -0.099      0.174     -0.571      0.568     -0.440      0.241\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lorenzo/code/notepad/.venv/lib/python3.8/site-packages/causalinference/estimators/ols.py:21: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  olscoef = np.linalg.lstsq(Z, Y)[0]\n",
      "/home/lorenzo/code/notepad/.venv/lib/python3.8/site-packages/causalinference/estimators/matching.py:100: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  return np.linalg.lstsq(X, Y)[0][1:]  # don't need intercept coef\n"
     ]
    }
   ],
   "source": [
    "causal_start = CausalModel(\n",
    "    Y=df_start[\"Y\"].to_numpy(),\n",
    "    D=df_start[\"D\"].to_numpy(),\n",
    "    X=df_start[[\"X0\", \"X1\", \"X2\", \"X3\"]].to_numpy()\n",
    ")\n",
    "\n",
    "causal_start.est_via_ols()\n",
    "causal_start.est_via_matching(bias_adj=True)\n",
    "print(causal_start.estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appliying only linear logistic regressions\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Estimated Parameters of Propensity Score\n",
      "\n",
      "                    Coef.       S.e.          z      P>|z|      [95% Conf. int.]\n",
      "--------------------------------------------------------------------------------\n",
      "     Intercept      1.724      1.791      0.962      0.336     -1.787      5.234\n",
      "            X0     -0.428      0.342     -1.254      0.210     -1.098      0.241\n",
      "            X1     -4.806      2.812     -1.709      0.087    -10.318      0.706\n",
      "            X2      0.071      0.052      1.366      0.172     -0.031      0.174\n",
      "            X3      2.747      1.832      1.500      0.134     -0.843      6.337\n",
      "\n",
      "Appliying linear logistic regressions and quadratic for X3 and X3*X1\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Estimated Parameters of Propensity Score\n",
      "\n",
      "                    Coef.       S.e.          z      P>|z|      [95% Conf. int.]\n",
      "--------------------------------------------------------------------------------\n",
      "     Intercept     -2.644      7.299     -0.362      0.717    -16.951     11.663\n",
      "            X0     -0.426      0.346     -1.232      0.218     -1.104      0.252\n",
      "            X1     -5.053     12.507     -0.404      0.686    -29.567     19.462\n",
      "            X2      0.076      0.053      1.425      0.154     -0.029      0.180\n",
      "            X3     14.448     15.297      0.945      0.345    -15.534     44.431\n",
      "         X3*X3     -7.832     14.256     -0.549      0.583    -35.773     20.110\n",
      "         X1*X3      0.008     16.480      0.000      1.000    -32.293     32.308\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Appliying only linear logistic regressions\")\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "causal_start.est_propensity(lin=\"all\")\n",
    "print(causal_start.propensity)\n",
    "\n",
    "print(\"Appliying linear logistic regressions and quadratic for X3 and X3*X1\")\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "causal_start.est_propensity(lin=\"all\", qua=[(3,3), (1,3)])\n",
    "print(causal_start.propensity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The search for a good combination of linear (`XN`) and quadratic (`XN*XN`) terms for the logistic regression to compute the propensity score is a foundamental way for understanding how the covariates influence each others [1][2].\n",
    "\n",
    "Remember that (see [link](https://stats.stackexchange.com/q/412668)):\n",
    "> Negative coefficients in a logistic regression model translate into odds ratios that are less than one (viz., (0,1)). That in turn, means that the predicted probability is decreasing as the covariate increases.\n",
    "\n",
    "Let's see how this number changes after intervention 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appliying only linear logistic regressions\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Estimated Parameters of Propensity Score\n",
      "\n",
      "                    Coef.       S.e.          z      P>|z|      [95% Conf. int.]\n",
      "--------------------------------------------------------------------------------\n",
      "     Intercept      1.404      1.797      0.781      0.435     -2.118      4.927\n",
      "            X0     -0.427      0.345     -1.237      0.216     -1.103      0.249\n",
      "            X1     -5.280      2.845     -1.856      0.063    -10.856      0.295\n",
      "            X2      0.067      0.053      1.276      0.202     -0.036      0.171\n",
      "            X3      3.947      1.848      2.136      0.033      0.325      7.569\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Appliying only linear logistic regressions\")\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "causal_interv1.est_propensity(lin=\"all\")\n",
    "print(causal_interv1.propensity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appliying linear logistic regressions and quadratic for X3 and X3*X1\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Estimated Parameters of Propensity Score\n",
      "\n",
      "                    Coef.       S.e.          z      P>|z|      [95% Conf. int.]\n",
      "--------------------------------------------------------------------------------\n",
      "     Intercept     -0.397      7.390     -0.054      0.957    -14.881     14.087\n",
      "            X0     -0.397      0.360     -1.100      0.271     -1.103      0.310\n",
      "            X1      1.554     13.822      0.112      0.911    -25.538     28.645\n",
      "            X2      0.053      0.054      0.968      0.333     -0.054      0.159\n",
      "            X3      1.106     14.977      0.074      0.941    -28.248     30.460\n",
      "         X3*X3      9.124     14.785      0.617      0.537    -19.854     38.103\n",
      "         X1*X3    -10.425     18.064     -0.577      0.564    -45.831     24.981\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Appliying linear logistic regressions and quadratic for X3 and X3*X1\")\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "causal_interv2.est_propensity(lin=\"all\", qua=[(3,3), (1,3)])\n",
    "print(causal_interv2.propensity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appliying Imbens-Rubin(2015) algorithm\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Estimated Parameters of Propensity Score\n",
      "\n",
      "                    Coef.       S.e.          z      P>|z|      [95% Conf. int.]\n",
      "--------------------------------------------------------------------------------\n",
      "     Intercept      0.998      1.567      0.637      0.524     -2.073      4.069\n",
      "            X1     -7.276      2.096     -3.471      0.001    -11.385     -3.167\n",
      "            X3      6.807      1.908      3.569      0.000      3.068     10.546\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Appliying Imbens-Rubin(2015) algorithm\")\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "causal_interv2.est_propensity_s()\n",
    "print(causal_interv2.propensity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note down the differences you notice from before and after intervention 2 and which covariates get influenced by the intervention in `X3`.\n",
    "\n",
    "### Trimming\n",
    "Set a cut-off value to drop values with the propensity score outside a given interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimated Parameters of Propensity Score\n",
      "\n",
      "                    Coef.       S.e.          z      P>|z|      [95% Conf. int.]\n",
      "--------------------------------------------------------------------------------\n",
      "     Intercept      2.435      1.522      1.600      0.110     -0.548      5.418\n",
      "            X1     -5.159      1.947     -2.649      0.008     -8.976     -1.342\n",
      "            X3      2.960      1.796      1.648      0.099     -0.560      6.480\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# default cut-off is 0.1: only PS between .1 and .9 are considered \n",
    "# because the min PS is 0.26, we need to set the cut-off high enough\n",
    "causal_start.cutoff = 0.30\n",
    "\n",
    "# before trimming\n",
    "causal_start.reset()\n",
    "causal_start.est_propensity_s()\n",
    "causal_start.trim()\n",
    "print(causal_start.propensity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset looks to be sensitive to dropping tail values.\n",
    "\n",
    "## Stratification\n",
    "Another usefull usage of the propensity score is to create buckets of records with inside some interval of PS. With stratification each block is a group in a propensity score interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stratification Summary\n",
      "\n",
      "              Propensity Score         Sample Size     Ave. Propensity   Outcome\n",
      "   Stratum      Min.      Max.  Controls   Treated  Controls   Treated  Raw-diff\n",
      "--------------------------------------------------------------------------------\n",
      "         1     0.405     0.624        34        41     0.537     0.548     0.228\n",
      "         2     0.629     0.904        21        53     0.696     0.729    -0.235\n",
      "\n"
     ]
    }
   ],
   "source": [
    "causal_start.reset()\n",
    "causal_start.est_propensity_s()\n",
    "\n",
    "causal_start.stratify_s()\n",
    "print(causal_start.strata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After stratification we are able to compute cleaner estimates for each stratum. This leads us to Blocking.\n",
    "\n",
    "## Blocking\n",
    "Aggregating strata estimates of treatment effects gives the *blovking estimator*, computed by the ATE of every stratum.\n",
    "\n",
    "With OLS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimated Parameters of Propensity Score\n",
      "\n",
      "                    Coef.       S.e.          z      P>|z|      [95% Conf. int.]\n",
      "--------------------------------------------------------------------------------\n",
      "     Intercept      2.435      1.522      1.600      0.110     -0.548      5.418\n",
      "            X1     -5.159      1.947     -2.649      0.008     -8.976     -1.342\n",
      "            X3      2.960      1.796      1.648      0.099     -0.560      6.480\n",
      "\n",
      "\n",
      "Treatment Effect Estimates: OLS\n",
      "\n",
      "                     Est.       S.e.          z      P>|z|      [95% Conf. int.]\n",
      "--------------------------------------------------------------------------------\n",
      "           ATE      0.083      0.101      0.818      0.413     -0.116      0.281\n",
      "           ATC      0.075      0.089      0.841      0.401     -0.099      0.249\n",
      "           ATT      0.088      0.114      0.771      0.441     -0.135      0.311\n",
      "\n"
     ]
    }
   ],
   "source": [
    "causal_start.reset()\n",
    "causal_start.est_propensity_s()\n",
    "causal_start.est_via_ols()\n",
    "print(causal_start.propensity)\n",
    "print(causal_start.estimates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Blocking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimated Parameters of Propensity Score\n",
      "\n",
      "                    Coef.       S.e.          z      P>|z|      [95% Conf. int.]\n",
      "--------------------------------------------------------------------------------\n",
      "     Intercept      2.435      1.522      1.600      0.110     -0.548      5.418\n",
      "            X1     -5.159      1.947     -2.649      0.008     -8.976     -1.342\n",
      "            X3      2.960      1.796      1.648      0.099     -0.560      6.480\n",
      "\n",
      "\n",
      "Treatment Effect Estimates: Blocking\n",
      "\n",
      "                     Est.       S.e.          z      P>|z|      [95% Conf. int.]\n",
      "--------------------------------------------------------------------------------\n",
      "           ATE      0.007      0.101      0.073      0.942     -0.190      0.205\n",
      "           ATC     -0.021      0.079     -0.269      0.788     -0.177      0.134\n",
      "           ATT      0.024      0.118      0.204      0.838     -0.208      0.256\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "causal_start.reset()\n",
    "causal_start.est_propensity_s()\n",
    "causal_start.stratify()  # without trimming\n",
    "causal_start.est_via_blocking()\n",
    "print(causal_start.propensity)\n",
    "print(causal_start.estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can se the debiasing effect performed by stratification and better estimates.\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "With these formally demonstrated tools we should be able to have a clearer understanding about the relations in place with the covariates, permutating over these exercise testing different assumptions can result into new debiased insights. With these new clues we can proceed more confident to a summary of our analysis or rather try to update our causal graph considering the influence that every variable can have on each other, maybe removing edges or establishing colliders and y-shapes in our graph. These methods for relations discovery can be automated via algorithms, that is what we are going to try in the next post.\n",
    "\n",
    "As an exercise you can try to answer why the causal graph below is, compared to the ones proposed in the previous posts, a better representations of the dataset according to what we have seen until here:\n",
    "\n",
    "![causal graph](./assets/CausalGraph-post-2.png)\n",
    "\n",
    "In particular can we weaken the probability of possible causal relation between `W` and `Y` according to the dataset?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "[1] Kuss, Oliver. “The z-difference can be used to measure covariate balance in matched propensity score analyses.” Journal of clinical epidemiology vol. 66,11 (2013): 1302-7. doi:10.1016/j.jclinepi.2013.06.001\n",
    "\n",
    "[2] Austin PC. An Introduction to Propensity Score Methods for Reducing the Effects of Confounding in Observational Studies. Multivariate Behav Res. 2011;46(3):399-424. doi:10.1080/00273171.2011.568786\n",
    "\n",
    "There are plenty of papers and dataset to tackle Causal Inference, you can start from [this list](https://github.com/matthewvowels1/Awesome-Causal-Inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a16d66621039bb8cee7875480a6bc62385a8d80ab6fd674ebc7420034665608f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
